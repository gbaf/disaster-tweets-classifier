{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "17a01ce1be814b7ce984159b2d13c220b107aee5165fc66b36fe58d0888abb6f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Disaster Tweets Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing all dependencies required for the notebook\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "source": [
    "# Data Exploration & Pre-processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data exploration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   keyword location                                               text  target\n",
       "id                                                                            \n",
       "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
       "4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n",
       "5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n",
       "6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n",
       "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/train.csv', index_col='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7613, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "df.shape # 7613 rows, with 4 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "# target 1 refers to disaster tweet, 0 is not a disaster tweet\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "61 rows have no keywords\n2533 rows have no location\n0 rows have no text\n0 rows have no target\n"
     ]
    }
   ],
   "source": [
    "# checking for completeness of data\n",
    "print(f\"{np.sum(df['keyword'].isna())} rows have no keywords\")\n",
    "print(f\"{np.sum(df['location'].isna())} rows have no location\")\n",
    "print(f\"{np.sum(df['text'].isna())} rows have no text\")\n",
    "print(f\"{np.sum(df['text'].isna())} rows have no target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "fatalities               45\n",
       "armageddon               42\n",
       "deluge                   42\n",
       "damage                   41\n",
       "body%20bags              41\n",
       "                         ..\n",
       "forest%20fire            19\n",
       "epicentre                12\n",
       "threat                   11\n",
       "inundation               10\n",
       "radiation%20emergency     9\n",
       "Name: keyword, Length: 221, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "# note that some keywords are phrases, with '%20' as a space\n",
    "df['keyword'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "USA                            104\n",
       "New York                        71\n",
       "United States                   50\n",
       "London                          45\n",
       "Canada                          29\n",
       "                              ... \n",
       "Port Charlotte, FL               1\n",
       "Dimapur                          1\n",
       "Orbost, Victoria, Australia      1\n",
       "(he/him)                         1\n",
       "Chicago, IL                      1\n",
       "Name: location, Length: 3341, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "# note that there are some non-location locations, like 'World Wide!!' and 'a feminist, modernist hag.'\n",
    "df['location'].value_counts() "
   ]
  },
  {
   "source": [
    "## Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download spaCy model for American English\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "source": [
    "## Modifying spaCy's tokenizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Token\t\tLemma\t\tStopword\n========================================\n2020\t\t2020\t\tFalse\nca\t\tcan\t\tTrue\nn't\t\tnot\t\tTrue\nget\t\tget\t\tTrue\nany\t\tany\t\tTrue\nworse\t\tbad\t\tFalse\n#\t\t#\t\tFalse\nihate2020\t\tihate2020\t\tFalse\n@bestfriend\t\t@bestfriend\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "# Let's see what spaCy does with numbers, contractions, #hashtags and @mentions\n",
    "s = \"2020 can't get any worse #ihate2020 @bestfriend\"\n",
    "doc = nlp(s)\n",
    "\n",
    "# Let's look at the lemmas and is stopword of each token\n",
    "print(f\"Token\\t\\tLemma\\t\\tStopword\")\n",
    "print(\"=\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Token\t\tLemma\t\tStopword\n========================================\n2020\t\t2020\t\tFalse\nca\t\tcan\t\tTrue\nn't\t\tnot\t\tTrue\nget\t\tget\t\tTrue\nany\t\tany\t\tTrue\nworse\t\tbad\t\tFalse\n#ihate2020\t\t#ihate2020\t\tFalse\n@bestfriend\t\t@bestfriend\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "# Contractions are split into lemmas\n",
    "# Numbers are their own features\n",
    "# @mentions are maintained as a token\n",
    "# We want to also keep #hashtags as a token, so we will modify the spaCy model's token_match\n",
    "\n",
    "import re \n",
    "\n",
    "# Retrieve the default token-matching regex pattern\n",
    "re_token_match = spacy.tokenizer._get_regex_pattern(nlp.Defaults.token_match)\n",
    "\n",
    "# Add #hashtag pattern\n",
    "re_token_match = f\"({re_token_match}|#\\w+)\"\n",
    "nlp.tokenizer.token_match = re.compile(re_token_match).match\n",
    "\n",
    "# Now let's try again\n",
    "s = \"2020 can't get any worse #ihate2020 @bestfriend\"\n",
    "doc = nlp(s)\n",
    "\n",
    "# Let's look at the lemmas and is stopword of each token\n",
    "print(f\"Token\\t\\tLemma\\t\\tStopword\")\n",
    "print(\"=\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "source": [
    "## Pre-processing a single tweet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original tweet: our deeds are the reason of this #earthquake may allah forgive us all\nToken\t\tLemma\t\tStopword\n========================================\nour\t\t-PRON-\t\tTrue\ndeeds\t\tdeed\t\tFalse\nare\t\tbe\t\tTrue\nthe\t\tthe\t\tTrue\nreason\t\treason\t\tFalse\nof\t\tof\t\tTrue\nthis\t\tthis\t\tTrue\n#earthquake\t\t#earthquake\t\tFalse\nmay\t\tmay\t\tTrue\nallah\t\tallah\t\tFalse\nforgive\t\tforgive\t\tFalse\nus\t\t-PRON-\t\tTrue\nall\t\tall\t\tTrue\nBag of words for the tweet: {'this': 1, 'allah': 1, 'forgive': 1, 'of': 1, '-PRON-': 2, '#earthquake': 1, 'all': 1, 'be': 1, 'reason': 1, 'may': 1, 'deed': 1, 'the': 1}\n"
     ]
    }
   ],
   "source": [
    "# Features is a set of all lemmas (words) encountered thus far\n",
    "features = set()\n",
    "\n",
    "# Now let's process an original tweet with our modified spaCy model\n",
    "s = df.loc[1,'text']\n",
    "print(f\"Original tweet: {s}\")\n",
    "\n",
    "# To lowercase\n",
    "s = s.lower()\n",
    "\n",
    "# Creating a doc with spaCy\n",
    "doc = nlp(s)\n",
    "\n",
    "# Let's look at the lemmas and is stopword of each token\n",
    "print(f\"Token\\t\\tLemma\\t\\tStopword\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "lemmas = []\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")\n",
    "    lemmas.append(token.lemma_)\n",
    "\n",
    "# Union between lemmas and our features set\n",
    "features |= set(lemmas)\n",
    "\n",
    "# Constructing a bag of words for the tweet\n",
    "freq = dict()\n",
    "for word in features:\n",
    "    freq[str(word)] = 0\n",
    "for token in doc: \n",
    "    freq[str(token.lemma_)] += 1\n",
    "    \n",
    "print(f\"Bag of words for the tweet: {freq}\")"
   ]
  },
  {
   "source": [
    "## Preprocessing all data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've preprocessed a single tweet, we can create a pre-process function for each tweet\n",
    "def preprocess(s, nlp, features):\n",
    "\n",
    "    # To lowercase\n",
    "    s = s.lower()\n",
    "\n",
    "    # Creating a doc with spaCy\n",
    "    doc = nlp(s)\n",
    "\n",
    "    # Let's look at the lemmas and is stopword of each token\n",
    "    print(f\"Token\\t\\tLemma\\t\\tStopword\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        print(f\"{token}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")\n",
    "        lemmas.append(token.lemma_)\n",
    "\n",
    "    # Union between lemmas and our features set\n",
    "    features |= set(lemmas)\n",
    "\n",
    "    # Constructing a bag of words for the tweet\n",
    "    freq = dict()\n",
    "    for word in features:\n",
    "        freq[str(word)] = 0\n",
    "    for token in doc: \n",
    "        freq[str(token.lemma_)] += 1\n",
    "        \n",
    "    return features, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_df = df #duplicate for preprocessing\n",
    "features = set() #using set feature to contain all words seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
       "\n",
       "[7613 rows x 0 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n    </tr>\n    <tr>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n    </tr>\n    <tr>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>2</th>\n    </tr>\n    <tr>\n      <th>3</th>\n    </tr>\n    <tr>\n      <th>4</th>\n    </tr>\n    <tr>\n      <th>...</th>\n    </tr>\n    <tr>\n      <th>7608</th>\n    </tr>\n    <tr>\n      <th>7609</th>\n    </tr>\n    <tr>\n      <th>7610</th>\n    </tr>\n    <tr>\n      <th>7611</th>\n    </tr>\n    <tr>\n      <th>7612</th>\n    </tr>\n  </tbody>\n</table>\n<p>7613 rows × 0 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "# create dataframe for bag of words representation\n",
    "bow = pd.DataFrame()\n",
    "bow['id'] = range(0, len(preprocess_df))\n",
    "bow.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to lower case\n",
    "preprocess_df['text'] = preprocess_df.text.map(lambda tweet: tweet.lower()) \n",
    "\n",
    "# create documents\n",
    "for i in range(0,1):\n",
    "    doc = nlp(preprocess_df.iloc[i]['text'])\n",
    "    #TO DO: build bag of words\n",
    "    features |= (set(doc)) #union tokens and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Saving pre-processed data for collaborators"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Splitting into training and validation data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Saving .csv files for training and validation sets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}