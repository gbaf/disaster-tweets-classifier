{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "17a01ce1be814b7ce984159b2d13c220b107aee5165fc66b36fe58d0888abb6f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Disaster Tweets Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing all dependencies required for the notebook\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "source": [
    "# Data Exploration & Pre-processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data exploration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/train.csv', index_col='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # 7613 rows, with 4 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 1 refers to disaster tweet, 0 is not a disaster tweet\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for completeness of data\n",
    "print(f\"{np.sum(df['keyword'].isna())} rows have no keywords\")\n",
    "print(f\"{np.sum(df['location'].isna())} rows have no location\")\n",
    "print(f\"{np.sum(df['text'].isna())} rows have no text\")\n",
    "print(f\"{np.sum(df['text'].isna())} rows have no target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that some keywords are phrases, with '%20' as a space\n",
    "df['keyword'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that there are some non-location locations, like 'World Wide!!' and 'a feminist, modernist hag.'\n",
    "df['location'].value_counts() "
   ]
  },
  {
   "source": [
    "## Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download spaCy model for American English\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "source": [
    "## Exploration: preprocessing a single tweet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to explore and preprocess a single tweet first\n",
    "s = df.loc[1,'text']\n",
    "print(f\"Original tweet: {s}\")\n",
    "\n",
    "# To lowercase\n",
    "s = s.lower()\n",
    "\n",
    "# Creating a doc with spaCy\n",
    "doc = nlp(s)\n",
    "\n",
    "# Let's look at the lemmas and is stopword of each token\n",
    "print(f\"Token\\t\\tLemma\\t\\tStopword\")\n",
    "print(\"=\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")\n",
    "\n",
    "features = set()\n",
    "features |= set(doc)\n",
    "\n",
    "# Constructing a bag of words for the tweet\n",
    "freq = dict()\n",
    "for word in features:\n",
    "    freq[str(word)] = 0\n",
    "for token in doc: \n",
    "    freq[str(token)] += 1\n",
    "    \n",
    "print(f\"Bag of words for the tweet: {freq}\")"
   ]
  },
  {
   "source": [
    "## Preprocessing all data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_df = df #duplicate for preprocessing\n",
    "features = set() #using set feature to contain all words seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for bag of words representation\n",
    "bow = pd.DataFrame()\n",
    "bow['id'] = range(0, len(preprocess_df))\n",
    "bow.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to lower case\n",
    "preprocess_df['text'] = preprocess_df.text.map(lambda tweet: tweet.lower()) \n",
    "\n",
    "# create documents\n",
    "for i in range(0,1):\n",
    "    doc = nlp(preprocess_df.iloc[i]['text'])\n",
    "    # TO DO: build bag of words\n",
    "    features |= (set(doc)) #union tokens and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Saving pre-processed data for collaborators"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Splitting into training and validation data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Saving .csv files for training and validation sets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}