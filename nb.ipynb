{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "17a01ce1be814b7ce984159b2d13c220b107aee5165fc66b36fe58d0888abb6f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Disaster Tweets Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing all dependencies required for the notebook\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "source": [
    "# Data Exploration & Pre-processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data exploration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   keyword location                                               text  target\n",
       "id                                                                            \n",
       "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
       "4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n",
       "5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n",
       "6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n",
       "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 163
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/train.csv', index_col='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7613, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "df.shape # 7613 rows, with 4 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "# target 1 refers to disaster tweet, 0 is not a disaster tweet\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "61 rows have no keywords\n2533 rows have no location\n0 rows have no text\n0 rows have no target\n"
     ]
    }
   ],
   "source": [
    "# checking for completeness of data\n",
    "print(f\"{np.sum(df['keyword'].isna())} rows have no keywords\")\n",
    "print(f\"{np.sum(df['location'].isna())} rows have no location\")\n",
    "print(f\"{np.sum(df['text'].isna())} rows have no text\")\n",
    "print(f\"{np.sum(df['text'].isna())} rows have no target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "fatalities               45\n",
       "armageddon               42\n",
       "deluge                   42\n",
       "damage                   41\n",
       "body%20bags              41\n",
       "                         ..\n",
       "forest%20fire            19\n",
       "epicentre                12\n",
       "threat                   11\n",
       "inundation               10\n",
       "radiation%20emergency     9\n",
       "Name: keyword, Length: 221, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "# note that some keywords are phrases, with '%20' as a space\n",
    "df['keyword'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "USA                            104\n",
       "New York                        71\n",
       "United States                   50\n",
       "London                          45\n",
       "Canada                          29\n",
       "                              ... \n",
       "Port Charlotte, FL               1\n",
       "Dimapur                          1\n",
       "Orbost, Victoria, Australia      1\n",
       "(he/him)                         1\n",
       "Chicago, IL                      1\n",
       "Name: location, Length: 3341, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "# note that there are some non-location locations, like 'World Wide!!' and 'a feminist, modernist hag.'\n",
    "df['location'].value_counts() "
   ]
  },
  {
   "source": [
    "## Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download spaCy model for American English\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "source": [
    "## Modifying spaCy's tokenizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - remove URLs using URL matcher \n",
    "# TODO - add '#' and '@' as single tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Token\t\tLemma\t\tStopword\n========================================\n2020\t\t2020\t\tFalse\nca\t\tcan\t\tTrue\nn't\t\tnot\t\tTrue\nget\t\tget\t\tTrue\nany\t\tany\t\tTrue\nworse\t\tbad\t\tFalse\n#\t\t#\t\tFalse\nihate2020\t\tihate2020\t\tFalse\n@bestfriend\t\t@bestfriend\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "# Let's see what spaCy does with numbers, contractions, #hashtags and @mentions\n",
    "s = \"2020 can't get any worse #ihate2020 @bestfriend\"\n",
    "doc = nlp(s)\n",
    "\n",
    "# Let's look at the lemmas and is stopword of each token\n",
    "print(f\"Token\\t\\tLemma\\t\\tStopword\")\n",
    "print(\"=\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Token\t\tLemma\t\tStopword\n========================================\n2020\t\t2020\t\tFalse\nca\t\tcan\t\tTrue\nn't\t\tnot\t\tTrue\nget\t\tget\t\tTrue\nany\t\tany\t\tTrue\nworse\t\tbad\t\tFalse\n#ihate2020\t\t#ihate2020\t\tFalse\n@bestfriend\t\t@bestfriend\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "# Contractions are split into lemmas\n",
    "# Numbers are their own features\n",
    "# @mentions are maintained as a token\n",
    "# We want to also keep #hashtags as a token, so we will modify the spaCy model's token_match\n",
    "\n",
    "import re \n",
    "\n",
    "# Retrieve the default token-matching regex pattern\n",
    "re_token_match = spacy.tokenizer._get_regex_pattern(nlp.Defaults.token_match)\n",
    "\n",
    "# Add #hashtag pattern\n",
    "re_token_match = f\"({re_token_match}|#\\w+)\"\n",
    "nlp.tokenizer.token_match = re.compile(re_token_match).match\n",
    "\n",
    "# Now let's try again\n",
    "s = \"2020 can't get any worse #ihate2020 @bestfriend\"\n",
    "doc = nlp(s)\n",
    "\n",
    "# Let's look at the lemmas and is stopword of each token\n",
    "print(f\"Token\\t\\tLemma\\t\\tStopword\")\n",
    "print(\"=\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "source": [
    "## Pre-processing a single tweet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original tweet: our deeds are the reason of this #earthquake may allah forgive us all\nToken\t\tLemma\t\tStopword\n========================================\nour\t\t-PRON-\t\tTrue\ndeeds\t\tdeed\t\tFalse\nare\t\tbe\t\tTrue\nthe\t\tthe\t\tTrue\nreason\t\treason\t\tFalse\nof\t\tof\t\tTrue\nthis\t\tthis\t\tTrue\n#earthquake\t\t#earthquake\t\tFalse\nmay\t\tmay\t\tTrue\nallah\t\tallah\t\tFalse\nforgive\t\tforgive\t\tFalse\nus\t\t-PRON-\t\tTrue\nall\t\tall\t\tTrue\nBag of words for the tweet: {'this': 1, 'allah': 1, 'forgive': 1, 'of': 1, '-PRON-': 2, '#earthquake': 1, 'all': 1, 'be': 1, 'reason': 1, 'may': 1, 'deed': 1, 'the': 1}\n"
     ]
    }
   ],
   "source": [
    "# Features is a set of all lemmas (words) encountered thus far\n",
    "features = set()\n",
    "\n",
    "# Now let's process an original tweet with our modified spaCy model\n",
    "s = df.loc[1,'text']\n",
    "print(f\"Original tweet: {s}\")\n",
    "\n",
    "# To lowercase\n",
    "s = s.lower()\n",
    "\n",
    "# Creating a doc with spaCy\n",
    "doc = nlp(s)\n",
    "\n",
    "# Let's look at the lemmas and is stopword of each token\n",
    "print(f\"Token\\t\\tLemma\\t\\tStopword\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "lemmas = []\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")\n",
    "    lemmas.append(token.lemma_)\n",
    "\n",
    "# Union between lemmas and our features set\n",
    "features |= set(lemmas)\n",
    "\n",
    "# Constructing a bag of words for the tweet\n",
    "freq = dict()\n",
    "for word in features:\n",
    "    freq[str(word)] = 0\n",
    "for token in doc: \n",
    "    freq[str(token.lemma_)] += 1\n",
    "    \n",
    "print(f\"Bag of words for the tweet: {freq}\")"
   ]
  },
  {
   "source": [
    "## Preprocessing all data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've preprocessed a single tweet, we can create a pre-process function for each tweet\n",
    "def preprocess(s, nlp, features):\n",
    "\n",
    "    # To lowercase\n",
    "    s = s.lower()\n",
    "\n",
    "    # Creating a doc with spaCy\n",
    "    doc = nlp(s)\n",
    "\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        lemmas.append(token.lemma_)\n",
    "\n",
    "    # Union between lemmas and our features set\n",
    "    features |= set(lemmas)\n",
    "\n",
    "    # Constructing a bag of words for the tweet\n",
    "    freq = dict()\n",
    "    for word in features:\n",
    "        freq[str(word)] = 0\n",
    "    for token in doc: \n",
    "        freq[str(token.lemma_)] += 1\n",
    "        \n",
    "    return features, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_df = df #duplicate for preprocessing\n",
    "features = set() #using set feature to contain all words (lemmas) seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
       "\n",
       "[7613 rows x 0 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n    </tr>\n    <tr>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n    </tr>\n    <tr>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>2</th>\n    </tr>\n    <tr>\n      <th>3</th>\n    </tr>\n    <tr>\n      <th>4</th>\n    </tr>\n    <tr>\n      <th>...</th>\n    </tr>\n    <tr>\n      <th>7608</th>\n    </tr>\n    <tr>\n      <th>7609</th>\n    </tr>\n    <tr>\n      <th>7610</th>\n    </tr>\n    <tr>\n      <th>7611</th>\n    </tr>\n    <tr>\n      <th>7612</th>\n    </tr>\n  </tbody>\n</table>\n<p>7613 rows × 0 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "# create dataframe for bag of words representation for each tweet\n",
    "bow = pd.DataFrame()\n",
    "bow['id'] = range(0, len(preprocess_df))\n",
    "bow.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'delmont', 'http://t.co/e1vkc2efst', 'http://t.co/jp2qlrunjj', '#photo', 'jax(mk2', 'mpp', 'ld0uniyw4k', '-&gt', 'stressful', 'http://t.co/fctrawjcyl', 'pass', 'sputtering', 'noxdv', 'av', \"f'in\", '@time', 'spain', 'industry', 'http://t.co/zhgu8ye1bj', 'http://t.co/j2erzbmjqd', 'fleet', '@scotrail', 'http://t.co/dyy7ml2nzj', 'looter', 'atmosphere', 'ooh', 'attractive', 'lean', 'veg', '#bluebell', 'like', '#eye', '@purpleturtlerdg', '@mirrorlady2', '#trucking', '3.1', 'ames', 'http://t.co/xrja0xpl40', 'derail', 'upcoming', 'lobby', 'literature', '#ices\\x89û_', '.@unsuckdcmetro', 'imaginable', 'http://t.co/tkjybjjskl', 'http://t.co/vlaiuvtomm', 'bigstar', '@lordofbetrayal', '@bobbyofhomewood', 'http://t.co/p769eo49fj', 'outta', '54.8', 'http://t.co/jnhnx3oisn', 'http://t.co/vmwtouyohm', 'ali', '@ebay', 'https://t.co/wkmfdig3nt', 'capitalism', 'recognise', 'amp;amp', 'http://t.co/fs4y1c9mnf', 'lane', 'probe', 'interesting', '@lifelettercafe', 'contig', 'smooth', '@fbi', 'compulsory', 'survive', 'majority', '7000-serie', '@folieacat', '@95roots', 'workspace', 'johnny', 'have', 'shevlin', 'https://t.co/yrqgnblkac', '@imawesome7986', 'http://t.co/lspnycvolo', '@mickinyman', '@kadiegrr', 'playoff', '@utahcanary', 'wristband', 'enjoy', 'emmerdale', 'saw', 'maxsys', 'http://t.co/yzzxxknism', 'http://t.co/inezzaes5d', 'dictator', '203rd', 'baaaack', 'http://t.co/ydgmgbryl2', '@flickershowell', 'loose', '.@fedex', '57', 'tg', 'declaration', 'https://t.co/iecc1jdoub', 'http://t.co/ij0wq490cs', 'http://t.co/irhh2gvse', 'http://t.co/zl7ojdaj3u', 'tub', 'three', 'warfighte', 'http://t.co/fce0k1ihti', 'hamburg', 'http://t.co/lxtjc87kls', 'http://t.co/u6isxv2f3v', 'autumn', 'scotiabank', 'http://t.co/cjctb2ocxg', '#latestnews', 'measures#arrestpastornganga', '88', 'todd', 'salvage', '@hunterlove1995', 'wales', 'damage', 'soccer', '@veronicadlcruz', '360', 'http://t.co/jn8s0drwbp', '1000', 'http://t.co/mtqjsvupwy', 'understanding', 'http://t.co/bclqpwfdod', 'lmaov.v', 'cobe', 'http://t.co/wtdiav11ua', 'torch', 'onion', 'tecno', 'http://t.co/nu6wrp716d', 'http://t.co/m0dap5xlwo', 'little', 'elementary', 'rename', '#baking', 'destructive', '@_oliviaann', 'neil', 'http://t.co/tagzbcxfj0', 'http://t.co/v5amdovhot', '#dance', '#job?', 'sleep', 'waiting', 'classic', 'deed', 'http://t.co/hbbgy2vzyt', 'coerce', '#cdnpoli', 'http://t.co/kgtxnnbj7y', 'epidemic', 'http://t.co/hxtupra5bc', 'http://t.co/5gktshiorr', 'springs', 'syndrome', 'variety', 'plane', 'myth', 'https://t.co/wcfpznsn9u', '#realestate', 'strange', 'fucking', 'contrie', 'pearl', 'gucci', '@reuter', 'https://t.co/nmgp7imeii', 'wank', 'http://t.co/shzsymausi', '1-russian', 'emotional', 'faintly', 'smell', '@basedlarock', 'indexing', 'http://t.co/tdhn9zy0er', 'realize', 'http://t.co/cm7hqwwulz', 'yd', 'http://t.co/dan0gkx28l', '#flood', 'abc', 'popular', 'ten', 'm1.94', '^oo^', 'disdain', 'whistle', 'http://t.co/havxobcsvu', '11', 'http://t.co/sgxb6e5yda', '@alanhahn', 'http://t.co/qsheu3yf0w', 'http://t.co/yzdmouxqbo', '@/@', 'divine', 'wedne', 'officer', '#preppertalk', 'https://t.co/rd10ex6hvt', '#fantasticfour', 'campaign', 'memories', 'safeco', 'aba', '#shipping', 'http://t.co/x6el3ysycn', 'osteen', 'mark', 'j', 'check', 'something', 'http://t.co/zarbwep9ld', 'cantar', '80s', 'http://t.co/gxe7nhwz3e', '1.43', '@teamstream', '#takecare', 'endless', 'resume', 'godlike', 'cena', 'class', ']', '#republican', 'economically', 'http://t.co/zpqwkhfhnf', 'electronic', 'feast', 'http://t.co/wqul8pg5px', 'warrant', 'trade', 'time2015', 'uplifting', '~', '#superfood', 'http', 'in', 'median', 'walk', '4500-feet', 'brewer', 'vitaly', 'rear', 'swiss', 'drink', '#endangered', 'hysteria', '#tubestrike', 'angeles', 'fiasco', 'http://t.co/1pdjoq4jc1', 'north', 'transgendere', 'arrival', 'committee', 'athletics', 'entrance', 'album', 'http://t.co/unhqcq6bex', 'okayyyyyy', '16:54:09', '@mallelis', 'http://t.co/btpqdehl3p', 'zayn', '1.9', 'northampton', 'tahoe', 'nuh', 'http://t.co/ch6e7vtatr', 'pbcanpcx', 'pisce', 'less', 'https://t.co/7spydy1csc', 'http://t.co/3tj8zjin21', '@darrellissa', 'monarchy', '@mapmyrun', \"1984'-style\", 'sentence', 'stripe', 'urgent', '@hvnewsnetwork', 'planetary', 'christ', 'cocaine', '@whippenz', 'https://t.co/0xouv7dhwz', '@khqa', 'mesick', 'lmfao', 'totteham', 'reportedly', 'german', 'sail', 'http://t.co/wmhy47xkil', 'broad', 'crime', 'full\\x89ã¢', 'http://t.co/l7bjsq0y2o', 'http://t.co/0yl3yt4ylh', '660', '@emilymcfcheslop', 'mile', 'greatly', 'http://t.co/8go68kje4b', 'truck', 'https://t.co/drno7okm21', '@4tpfa', 'perfectly', '#orchardalley', 'yourself', '0.6', 'delhi', 'country', 'u\\x89û', '#dynamix', '#rtrrt', 'fun', '@bbctalkback', '@adamtuss', '-tune', 'gmmbc', '@magnum', 'box', '#uk', 'http://t.co/ga14egplw9', 'http://t.co/jlwhaowfqa', 'emilee', 'http://t.co/6ffylajwps', 'morn', 'off', '@demetae12', 'premiere', 'yours', 'https://t.co/0mcxc68gzd', 'pepper', 'excite', 'silvery', 'http://t.co/i9mxxkzhbl', 'bull', 'overtime', 'invasion', 'beside', 'http://t.co/vrpmplczcy', '@cod8sandscrims', '#decisionsondecisions', 'surprise', 'http://t.co/nnv3zwvant', 'cannon', 'notice', 'sitting\\x89û', 'http://t.co/ykvsttvdwo', 'telnet', '@crystal_blaz', 'http://t.co/wadpp69lwj', '@kalinandmyle', '91å¡f', 'snuck', 'http://t.co/cgdj3drso9', 'warfighting', 'wall', 'http://t.co/mwzqcjhzpb', 'colour', 'http://t.co/dkozymvy5l', 'http://t.co/jaxhzjccd4', '#technology', 'derivative', 'limit', 'http://t.co/facm78eg7k', 'adelaide', 'extreme', 'allegiance', '#education', 'introduction', 'infantry', 'same', 'hotel', 'e(oficial', 'react', 'wrap', 'http://t.co/plo2qkrwhu', 'noches', 'humboldt', 'jhaustin', 'http://t.co/cfouwpbrcg', 'cooool', '#israeli', 'sony', 'https://t.co/2azxdlcxga', 'poll', '.@aphl', 'drool', 'fucktard', 'grandma', '@bbctms', 'fanfiction', 'peace\\x89ûª', 'president\\x89û\\x9d', 'absurdly', 'tomorrow', 'dismayed:\\x89û', '141', 'us101', 'http://t.co/rj7m42atws', 'pitch', 'trfc', '\\x89ûï@leoblakecarter', '@theadvocatemag', 'petersburg', 'http://t.co/ml8irhwg7o', 'campus', 'accepte', 'infected', 'anna', '@suelinflower', '662', 'http://t.co/gan14pw9tg', 'clelli', 'http://t.co/ypbvs1ijya', '#imkeepingmydayjob', 'advice', 'http://t.co/aptap6yx1r', 'har', 'rio', 'minute', '@abysmaljoiner', '@dustpiggies', 'pissed', 'concrete', 'http://t.co/1osmiuxkhw', 'blanket', 'https://t.co/xmwodfmtui', '#demolished', 'comprehensive', 'upset', 'http://t.co/phixznv1yn', 'creation', 'marks', 'straight', '033', 'invite', 'launch', 'http://t.co/q8reoevlue', 'http://t.co/2fenu1syu6', 'tactful', 'pyrotechnic', 'mizuta', 'tenn', '9/29', 'mma', '@steel_lord', 'famine[mega', '@rorington95', '#demolition', 'death', 'next\\x89û', 'snd', 'http://t.co/6qc8whdizy', '#infectiousdisease', 'mkayla', '@cumcovered', 'storey', 'trim', 'http://t.co/dy1ersdcrh', 'lez', 'associate', '#yolandaph', 'morty', 'healthy', '#innovation', 'screw', 'http://t.co/mfjxh4p51u', 'http://t.co/ltvvpflsg8', '@crude', 'gyrsi', 'atlanta', 'computer', 'åç', '@kalinwhite', 'http://t.co/jbjrg3ep1q', 'unpredictable', 'morgan', 'appropriate', 'boyhaus', 'shaw', 'raider', 'mre', '@blizzard_gamin', '#ootd', 'mumbai24x7', 'choke', 'missing', '#fedex', 'evening', 'http://t.co/5cm0lfzhxn', 'ferguson', 'sensitive', 'crbzfz', 'esteemed', 'sexy', '@washingtonpost', '@chr3lyc', 'misfortunebut', 'actually', 'http://t.co/jzgxwrgfqg', 'ebay', 'navy', '#red', 'drift', 'lonely', 'pill', 'ferrochrome', 'sneeze', 'battery', 'house', '@arianareed11', 'alexis', 'silent', 'foolish', 'greg', 'mining', '#gadget', 'thursday', '#0215', '#plague', 'syrian', '08', '501', 'mechanic', '#edwing', 'ronaldo', 'mistake', 'jar', 'mad', 'http://t.co/bqbhuyfme9', 'such', 'hashimoto', 'own', '@xgn_infinity', 'client', 'http://t.co/uppwxda4', 'http://t.co/adsvdpnp3r', 'skype', '#irandeal', 'cessna', 'excited', 'ur', 'moe', '#whatcanthedo', 'centre', 'http://t.co/mt029qj4ig', 'http://t.co/8kjs7zqajs', 'queen', '@joshcorman', 'strengthen', 'dhsscitech', 'small', 'proxy', 'rijn', '@sweetiebirk', '#shelli???', '01:50:25', '@christinalavv', 'http://t.co/ajtxuafoem', 'http://t.co/gx1otoh8sj', 'depot', 'http://t.co/targ56igbz', '@cyclone_reizei', 'dust', 'http://t.co/3xn1soh4bb', '#blackinamerica', '@barbi_twins', 'royal', 'stall', 'county', 'couple', '#bancodeseries', 'genuine', 'weaken', 'ursula', 'http://t.co/raf732vrtt', 'http://t.co/c1qxjweqqu', 'month', '\\x89ûï@lolgop', 'http://t.co/2fs649qdwx', 'http://t.co/vox99fwkcx', 'let\\x89ûª', 'airing', 'astonishing', 'tunas', '@nicklee8', 'http://t.co/p6cylz5lpt', 'http://t.co/naex0q1ax0', 'along', '@o_magazine', 'gore', 'selfavowed', 'brig', 'listing', 'hill', 'kick', 'wolter', 'https://t.co/gi2p9tuvbi', 'http://t.co/huriivfdkc', 'dystopian', '#wniagospel', 'eto', '@playstation', 'equal', 'starbs', 'amiibos', 'sustainourearth', '@torontorc', 'yumiko', 'http://t.co/77ciwxabva\\x89û_t-school-lands/', '@slatukip', 'http://t.co/mr5bi4kd82', '#gbbo', 'compare', 'sketchbook', 'https://t.co/p1amginsys', '#artistsunite', 'en', '#traintragedy', 'ticket', '#fly', 'half', 'mess', '29/20', 'https://t.co/cvkqigr1az', 'kaiser', 'raid', 'lzkely', 'https://t.co/drfkarlz1d', 'earthquake', 'illness', 'https://t.co/kq9ae6ap2b', 'ridiculous', '@violentfeminazi', 'http://t.co/jxhazebnqk', 'trump', '#rochdale', '#boycottbears', 'simply', 'i-405', 'http://t.co/hfy5v3slbb', 'drill', '@imagecomics', 'dc', 'stl', 'doone', '@treyarch', 'underwater', '#southkorea', 'https://t.co/cyompz1a0z', 'flash', 'urs', 'prince', 'boone', 'zarharzar', 'nbc', '#hiphop', 'suite', 'heartless', '#asap', 'https://t.co/zhjlflbhzl', 'snuff', 'significant', '\\x89ûï#hannaph\\x89û\\x9d', 'poplar', 'delete', 'http://t.co/93iaeec26', '#gishwhe', 'forebode', 'shoook', 'fwy', 'urself', 'loan', 'http://t.co/gjbainqwn9', '#biztip', 'http://t.co/zckxtfc9pt', 'clipuri', 'http://t.co/lvkoemsq8', 'canvas', 'isle', 'contractor', 'puncture', '#backtoback', 'anxious', 'palestinian', '@ethereal_7', 'heavy', 'isil', '@djjohnblaze', 'http://t.co/jsuhupz6vp', 'nose', 'whitbourne', 'level', 'morebut', '@sonofbaldwin', '#poze', 'vabengal', 'http://t.co/kshsgwghfj', 'wife', 'hey!sundowns', '@i_amtalia', 'http://t.co/pmxezuo4ay', 'wouldn\\x89ûªt', 'http://t.co/lgglf5yrme', '2065', 'http://t.co/phnez60cwe', 'more', '#disasterrecovery', 'worsen', 'http://t.co/wwgadpffkw', '#krefeld:', 'http://t.co/namffldh5h', 'goonda', 'hug', 'fill', 'offense', 'ssp', 'http://t.co/c8uxkizwm6', 'http://t.co/zoepzsoky1', '#eyewitness', 'tragic', 'washington', 'stir', 'exterminate', 'http://t.co/ba2rrxugsg', 'accident', '#escort', 'eventually', 'independence', 'refund', 'http://t.co/vbo1tjndps', 'cancer', 'http://t.co/toyu16mxbo', '49', '#nh1new', '.....', 'http://t.co/wpdvt31sne', 'one', 'zodiac', 'procedure', 'luke', 'cst', 'http://t.co/st5fgblsye', 'yell', 'david\\x89û', 'karnal', 'http://t.co/iucpcsfbmt', 'rapidly', 'scene', '@damnaarielle', 'http://t.co/8j09zutxwt', 'isla', '401ks', 'birb', 'qualit', '87', 'loo', 'extinguish', 'grave', 'detonate', '@annihilation', 'allegation', 'http://t.co/p9rymfjcux', 'http://t.co/l1gfxgozvx', 'detector', 'agreement', 'dynastic', 'http://t.co/oqpsvrgbjc', 'maddddd', 'hair', 'counterstrike', '13', 'hit', 'armory', 'blazing', 'afte', '@allahsfinest12', 'palestine', 'alternate', 'hump', 'glad', 'weekend', '\\x89û÷second\\x89ûª', '#newep', '#thingsihate', 'apologise', 'we\\x89ûªre', '@kircut1', 'https://t.co/wemwdtfwic', 'cost\\x89û', 'http://t.co/rbmucurs2f', '@indigo6e', 'http://t.co/jixscpmdud', 'http://t.co/ha5boppejy', 'cas', '@funkylilshack', 'krefeld', 'commercial', 'm-115', 'site', 'armed', 'feminist', 'afraid', 'ridah', '^mp', 'jewish', 'http://t.co/hcyajsacfj', 'slash', 'tram', 'basic', 'http://t.co/bbm9sr1wow', 'californian', 'dtn', '@safyuan', 'http://t.co/qzvkpahsq7', 'oral', 'http://t.co/sslt8esmhy', '@deniseromano', 'http://t.co/cubze5mizm', '#tragedy', '#dc', 'end', 'world', '                ', 'trauma', 'father', 'africaå¨', 'motivator', 'anger', 'cree', '@healthweekly1', 'http://t.co/gvaipmlsl0', 'value', '34:22', '#overwatch', 'http://t.co/wuicdttuhf', '90', '@beautiful_juic1', 'http://t.co/iylvzy3cob', '   ', 'veil', 'accord', '@charstevens97', 'xb1', 'vandalize', 'https://t.co/ciyty0fgpr', 'taylor', 'etoffe', 'coat', 'midget', '@newyorkcity', '#dumle', 'finance', 'tired', '.@uber', 'specialguest', 'stearns', '6.beyonce', 'form', 'ii', 'womengirl', '0.9', 'crematoria', 'maj', 'm4', 'khuzdar', 'annihilate', 'co', 'http://t.co/0lmheaex9k', 'actively', 'zombie', 'corpse', 'excellence', 'http://t.co/lpgfqnpjd3', 'quiz', 'http://t.co/sywuel7yyx', 'insas', 'http://t.co/s0vgtkhw7v', '@ebrointheam', '#newsintweets', 'http://t.co/2pimg9bice', 'oworoshoki', '@adrian_peel', 'http://t.co/4kdthctemv', '#allthenew', 'dead', 'blaze', 'lucio', 'so\\x89û', 'wear', 'effort', 'anyways', 'bathe', 'http://t.co/sb5r7shccj', 'florida', '@officeofrg', 'deserve', 'riot', 'http://t.co/ieksnfsby7', '#yes', 'http://t.co/28t3nwhdky', 'http://t.co/t344phnpy9', '#digitalhealth', 'http://t.co/vnzybfgzcm', 'norfolk', 'http://t.co/sdgoutwntb', 'http://t.co/oogoto76uz', 'totalitarianism', 'quarter', '@graysondolan', 'mannered', 'gon', 'http://t.co/wbcb3sytj7', \"'\", 'and', 'misfit', 'http://t.co/dqxkjibbky', 'approval', 'https://t.co/k5knnwugwt', 'ks111', \"#nevada's\", '@mofanon', 'julian', 'open', 'supposedly', 'carpet', '\\x89û÷institute', 'http://t.co/zxcorq0a3a', '#followme', 'halt', 'http://t.co/mgr809yc5a', '@acebreakingnews', 'raw', '@mediaguido', '12:32', '@ezralevant', 'wise', 'africans', 'study', 'http://t.co/7mepkbf9e8', 'propose', 'mrc', '@tinyjecht', 'cara', '#promo', '#contentmarketing', 'bid', 'rover', 'tribez', 'american', 'behold', 'owner', 'transcend', 'http://t.co/j6mpdsx9lk', 'http://t.co/wgwiqmicl1', 'http://t.co/skqpwsnoin', 'uganda', 'http://t.co/6odnbttpsq', 'shop\\x89ûªs', 'http://t.co/uqssnaattu', 'throw', 'backyard', 'almost', '@amateurnester', 'helens', 'hoax', 'wom', 'osage', 'painful', 'squeeze', 'tone', 'occupant', '@spookyfob', 'point', '22.beyonce', 'those', 'cam', '#desolation', 'normal', 'http://t.co/pqhuthss3i', 'maintenance', 'lovely', 'mwjcdk', 'utc2015', 'obviously', 'milkshake', '\\x89û÷amino', 'http://t.co/dfyasvj7nf', 'feeling-', '#fashion', 'ik', 'antonio', '#yemen', 'spread', 'https://t.co/czqr3ci9xw', 'course', 'hinata', '#carfest', 'stewart', '.doc', 'rite', 'iclown', 'dick', '#glaucoma', 'company', 'noise', 'maiga', 'intl', 'thanks', 'scary', 'terrorist', 'subcommittee', 'http://t.co/7lvgcmyiyj', 'act', '#exec', 'macia\\x89ûªs', 'creativity', 'http://t.co/pbclfsxrld', 'https://t.co/7vsqqsvgni', '#wish', 'mukilteo', 'http://t.co/iadlslqdpd', 'nah', 'http://t.co/cxdjapzxmp', 'troy', 'progressive', '@scotto519', 'https://youtu', 'page', 'sting', 'status', 'brothers', \"took'em\", 'streamyx', '@femalegilgamesh', 'potato', '@cjoyner', 'grrrr', 'http://t.co/cznxhutasx', 'passive', 'ghost', 'carry', 'wrong', '@geoffrickly', '@sholt87', 'tutorial', 'interstate', 'long', 'valuation', 'troupe', 'contribute', 'ouvir', 'https://t.co/ipgmf4ttdx', 'political', 'https://t.co/sg1ftkaegq', 'http://t.co/9fxmn0l0bd', 'cilla', 'http://t.co/ypvvqz8jzt', 'ph0tos', 'increasingly', 'connection', 'fall', 'awesome', 'vault', '#donzilla...', 'michael', 'http://t.co/tjgxdc3d5p', 'burglary', '#nursing', '#india', '@_301dc', '420', 'almighty', 'propel', 'substance', '@sensanders', 'cross', 'johns', 'view', 'http://t.co/tmmorvxswz', 'peasant', 'kanger', 'womens', 'https://t.co/uttanbigrx', 'friendship', 'misstep', '#mumbai', 'approach', 'http://t.co/bxalnedy49', 'http://t.co/wnuqqatttp', 'blvd', 'wii', 'sequence', 'scenario', 'http://t.co/7jggqwbv6s', 'conf\\x89û', '#airport', 'worthless', 'news3lv', 'quite', 'insert', '@starbuck', 'attendance', '#pieceofme', 'football', 'shadowman', 'desperation', 'chop', '@wwwbigbaldhead', 'googlemaps', 'strategy', 'muscle', 'smfh', 'collapse', 'mph', '#mexico', 'blue', '#meditationbymsg', 'http://t.co/htez4z48od', 'no', 'michele', 'butiqob', 'ankle', 'rockin', 'bowhunte', 'conservative', 'http://t.co/bmxsndx14', 'eve', 'assault', '5000', '@renunciedilma', 'track', 'compassion', 'http://t.co/o7qogmoegu', 'i-75', 'shooter', 'content', 'western', 'beach', '#lotz', 'sad', 'holland', 'faan', 'exp', 'http://t.co/l5vaklr59', '@nuggets', 'http://t.co/tgcr5vofj3', 'catfish', '1967', '241487', 'daniel', 'turkey', 'http://t.co/s4sicmyrmh', 'european', '#auction', 'nicole', '101', 'major', 'nepal', 'rs', 'austrian', 'intertwine', 'killsåêone', 'http://t.co/kolmzbz1pz', '#crossfit...', 'evanston', 'conservation', '#doomsday', 'oz', 'i-65', 'alexandrian', 'tutor', 'chinese', 'the_af', 'quickly', 'http://t.co/spymiqnecj', 'crush??', 'g3', 'trafford', 'keegan', 'http://t.co/anp9g6njfd', 'billneelynbc', 'http://t.co/mzqq5pai8', 'http://t.co/4ryzmzsgdw', '@joonma', '\\n    ', 'individual', 'https://t.co/ycwaulqz3u', 'lt;b&gt;chocolate&lt;/b&gt', 'abuse', 'http://t.co/r5bxzzhxkm', 'aust', 'lone', 'fnaf', 'dirt', 'never', 'pharaoh', 'emile', '@bellalinn', 'catastrophe', 'witch', '#genocide;', '#avalanche:', '1.3', 'arabia', '#avigdorliberman', 'renison', '@gregorysanders', 'aspire', 'force', 'normally', 'collided', 'forgive', 'mood', 'bairstow', 'mop', 'dinner', 'give', 'exclusive', 'moves:/', 'tattoo', 'acquisition', '@cumtown', 'najib', 'mango', 'famine', 'http://t.co/wgasoencwc', 'tyrone', 'http://t.co/a2ss7pr4gw', 'ioqm5bm1dg', 'sympathy', '1200', 'https://t.co/vnni3zzuz6', 'mirkwood', '@sanjaynirupam', '138:8', 'tend', 'lyme', 'cousin', 'blessing', 'http://t.co/f4xnplio5s', 'moseby', 'sprinter', 'rea\\x89û', \"gov't\", '@eazzy_p', 'http://t.co/2sz7okjrxi', '@annealiz1', 'style', '@retiredfilth', '@cameronciletti', '#pt', 'hairdryer', 'aggressif', 'naayf', 'http://t.co/wrb7xd8w5y', 'second', '#dw', '#abandone', 'everythign', '@autoames', 'http://t.co/dxkt2shuj2', 'swimming', 'ml', 'http://t.co/pxyh7zo7vt', 'bored', 'timestack', 'necessary', '11:00', 'ssw', '@greenharvard', '|via', 'coffin', 'reading', 'ricket', 'ant', 'nort', 'condolence', 'boss', 'http://t.co/moa0q0aufa', 'patna', '1028', 'jonesboro', 'http://t.co/g891m9gh4r', ';', 'student', 'http://t.co/6gldwx71da', 'http://t.co/qrhjei7grq', 'electr', '-sister', 'stars', 'emts', 'http://t.co/wq3wjsgphl', 'face', '#loseit', 'https://t.co/ecmujkkqx1', 'http://t.co/otsm38xmas', '#shoes', 'http://t.co/mjfdcrjs8j', 'kinder', 'relive', 'http://t.co/fven1ex0ps', 'ghe', '@mrtophypup', '@msmigot', '@tammy_w1997', '@rtrrtcoach', 'http://t.co/r1xwdnvb0', 'museum', 'http://t.co/yvsjyzwxjr', 'mount', '@yahoonews', 'pamela', 'http://t.co/k9cd0efvut', '@aliveafter5', 'marlon', '@blazerfan', 'salty', 'kai', 'ted', 'novel', 'ye', 'http://t.co/i2aag6lp6w', 'ch', \"onåê'the\", 'matias', '#euroquake', 'families', 'http://t.co/nq89drydbu', 'typical', 'http://t.co/950xijhnvh', 'genisys', 'horror', 'undergroundbestseller', '#wave', 'dryer', 'geller', 'cobra', 'ouch', 'mencius', '#iwontmakeit', 'cherry', 'http://t.co/h4usheekxm', '90blks&amp;8whts', 'slideshare', 'http://t.co/nyp8oq2z9', 'i-24', 'http://t.co/vq3vwxj8yu', '#oomf', 'unknowingly', 'ave', 'usama', 'land', 'recovery', 'http://t.co/sxqtyduvul', 'http://t.co/blzt5zwose', 'lt;a', 'notification', 'rear-', '@keren_serpa', 'bute', 'coiffed', 'ricin', 'wish', 'only', 'available', '#turbojet', 'murlo', 'tsunami', 'phew', 'http://t.co/ag65u29sgo', 'refugee', '@bigsim50', '@playoverwatch', '@_chelsdelong12', '#calfire', 'http://t.co/l0pmmtzlwp', 'pt1', '#foxnews', '#vra50\\x89û\\x9d', 'float', 'hw18', 'http://t.co/nlfr8t3xqm', 'tte', 'era', 'universe', 'hazmat', '@popehat', 'http://t.co/paobghv3c7', \"talk='ecology'&amp;'human\", 'http://t.co/pwd5l0glkv', 'walker', 'social', 'resources', 'which', 'http://t.co/pe2d3hcsni', 'http://t.co/fnmje8gf7', 'ethereal', 'personalize', 'outlet', 'throwin', '#nowplay', '#computers', 'https://t.co/jv8ppkhjy7', 'conflict', 'colin', \"we're\", 'lay', 'dragneel', 'shud', '#nyc', '500th', 'http://t.co/6uhih9pbru', 'penn', 'aggressive', 'http://t.co/2o7eva1coe', 'toenail', 'max', 'http://t.co/ppji1tcnml', 'inc', '@robertmeyer9', 'sunnis', 'worry', 'http://t.co/a5gnzbusqq', 'lego'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):#len(preprocess_df)):\n",
    "    features, bow = preprocess(preprocess_df.iloc[i]['text'],nlp,features)\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Saving pre-processed data for collaborators"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Splitting into training and validation data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Saving .csv files for training and validation sets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id\n",
       "0   1\n",
       "1   2\n",
       "2   3\n",
       "3   4\n",
       "4   5"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 188
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}